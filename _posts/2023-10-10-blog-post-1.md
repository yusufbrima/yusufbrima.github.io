---
title: 'K-Nearest Neighbors from First Principles'
date: 2023-10-10
permalink: /posts/2023/10/blog-post-1/
author: Yusuf Brima
tags:
  - Classification
  - Representation Learning
  - Metric Learning
  - Machine Learning
  - Unsepervised Learning
---
<p class="page__date"><strong>
  <i class="fa fa-fw fa-user" aria-hidden="true"></i> Author:</strong>
  Yusuf Brima
</p>

Introduction
======
In this post, I will walk through how to implement the K-Nearest Neighbors (KNN) machine learning algorithm from scratch in Python. KNN is one of the most fundamental ML algorithms used for classification and regression.

Understanding KNN 
======
The intuition behind KNN is quite simple. Given a new data point, I look at its closest $K$ neighbors in the training data and predict the label based on those neighbors. For classification, I take a majority vote of the neighbor labels. For regression, I take the average of the neighbor values.

More formally, here are the steps I follow to apply KNN:

1. I choose a value for K - the number of neighbors to examine.
2. I calculate the distance between the new data point and all points in the training set using a [distance metric](https://en.wikipedia.org/wiki/Distance_metric) like Euclidean or Manhattan distance. 
3. I sort the distances and take the top K closest points.  
4. For classification, I take the majority class label of the $K$ points. For regression, I take the average value of the K points.

An illustration of the KNN algorithm for a sample 2D dataset is shown below:

<figure id="KNN_Illustration">
    <img src="http://yusufbrima.github.io/images/KNN_Illustration.jpg" style="height:500px;width:680px;"
         alt="Euclidian distance in 2D">
    <figcaption> Figure 1: (Left) A 2D feature visualization of a dataset that comprises three classes ($C_1, C2, C_3$). The colors indicate the class labels. With a new datapoint $X_\mathrm{new}$, the objective of the K-NN algorithm is to find the class $C_i$ to which the datapoint can be classified based on its $K$ neighbors labels. (Right) After computing the distance between the query $X_\mathrm{new}$ and all the samples in the dataset, it has been classified into class $C_3$ which comprises a majority of the new sample neighbors..
    </figcaption>
</figure>

The key steps are computing distances, finding neighbors, and aggregating labels. The effectiveness of KNN depends heavily on the choice of distance metric and value of K.

Implementing KNN Classifier in Python
======

I will now walk through a simple example of implementing a KNN classifier in Python for classifying 2D points.

First I generate some sample 2D data:

\`\`\`python
import numpy as np
from sklearn.datasets import make_blobs

X, y = make_blobs(n_samples=500, centers=2, random_state=42)
\`\`\`

For 2D data, I use the Euclidean distance metric. The implementation is straightforward:  

\`\`\`python
import math

def euclidean_distance(a, b):
  distance = 0
  for i in range(len(a)):
    distance += (a[i] - b[i])**2
  return math.sqrt(distance) 
\`\`\`

To find the K nearest neighbors, I calculate distances to all points, sort them, and take the first K:

\`\`\`python  
import operator

def get_neighbors(training_set, new_point, K):
  distances = []
  for x in training_set:
    dist = euclidean_distance(x, new_point)
    distances.append((x, dist))

  distances.sort(key=operator.itemgetter(1))
  
  neighbors = []
  for x in range(K):
    neighbors.append(distances[x][0])
    
  return neighbors
\`\`\`

For classification, I take a majority vote among the K neighbors:  

\`\`\`python
import collections

def predict_classification(training_set, new_point, K):
  neighbors = get_neighbors(training_set, new_point, K)
  output_values = [x[-1] for x in neighbors]
  prediction = collections.Counter(output_values).most_common(1)[0][0]  
    
  return prediction
\`\`\`

And we have a basic KNN classifier in Python! With just a few lines of code, I have implemented the core ideas behind KNN - computing distances, finding neighbors, and aggregating their labels. 


Choosing K and Distance Metrics
======

The two most important design choices with KNN are selecting the value of K and the distance metric. Finding the optimal values requires experimenting on a validation set.

Some guidelines on choosing K and distance metrics:

- K controls the tradeoff between bias and variance. Larger K reduces variance but increases bias. [Hyperparameter optimization](https://machinelearningmastery.com/hyperparameter-optimization-for-machine-learning-models/) can help choose the ideal K.
- Distance metrics depend on the data type. [Euclidean works best](https://en.wikipedia.org/wiki/Euclidean_distance) for low dimensions. [Manhattan](https://en.wikipedia.org/wiki/Taxicab_geometry) or [Hamming](https://en.wikipedia.org/wiki/Hamming_distance) are better for high dimensions. 
- Feature scaling is important before computing distances.
- For weighted features, use a weighted distance like [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance).

When KNN Shines  
======

KNN tends to perform very well when:

- The data has low inherent dimensionality  
- The decision boundaries between classes are irregular
- There is plentiful training data compared to dimensionality

KNN struggles when:

- Data has very high dimensionality (curse of dimensionality) 
- Classes are well-separated
- Training data is sparse
- Need to extrapolate beyond training data  

In many cases, other algorithms like SVM, random forests, and neural nets outperform KNN, especially on complex modern datasets. But KNN is still widely used for its simplicity, interpretability, and surprisingly good performance.

Summary  
======

In this post, I walked through implementing K-Nearest Neighbors from scratch in Python based on first principles. KNN exemplifies the powerful "learning by analogy" paradigm underlying many ML algorithms. With just distances and neighbors, KNN can deliver excellent results on many problems while being simple to understand and implement. While more complex techniques like deep nets are popular today, starting with fundamentals like KNN is key for any practitioner looking to really grok machine learning.  


Acknowledgement
======
Thanks to [Dr. Ulf Krumnack](https://www.ikw.uni-osnabrueck.de/en/research_groups/computer_vision/people/krumnack_ulf.html) for his insightful feedback while developing this post.

