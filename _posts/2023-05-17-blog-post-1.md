---
title: 'K-Means Clustering from First Principles'
date: 2023-05-17
permalink: /posts/2023/05/blog-post-2/
author: Yusuf Brima
tags:
  - Clustering
  - Representation Learning
  - Metric Learning
  - Machine Learning
  - Unsepervised Learning
---
<p class="page__date"><strong>
  <i class="fa fa-fw fa-user" aria-hidden="true"></i> Author:</strong>
  Yusuf Brima
</p>

Introduction
======
In today's data-driven world, uncovering hidden structures within complex datasets is a crucial task, whether it's analyzing customer behavior, identifying disease patterns in medical data, or classifying text documents. <a href="https://en.wikipedia.org/wiki/Machine_learning" target="_blank"> Machine learning (ML) algorithms</a> offer powerful tools to navigate the intricacies of data analysis. Boardly speaking, these ML algorithms can be categorized into Supervised Learning, Unsupervised Learning, and Reinforcement Learning. In this post, we will focus on the unsupervised learning algorithm, K-means clustering. This algorithm has recently gained tracktion in the field of <a href="https://en.wikipedia.org/wiki/Feature_learning" target="_blank">representation learning also know as feature learning</a>, where it is used to learn a representation of the data. This versatile technique has found applications in various domains, addressing challenges like market segmentation, anomaly detection, image recognition, and recommendation systems. By automatically identifying groups of similar data points based on their features, K-means clustering enables efficient customer segmentation, personalized healthcare approaches, and effective text categorization. It leverages computational power to unveil valuable patterns and provides insights that may not be apparent at first glance, empowering businesses, researchers, and analysts to make informed decisions and drive innovation.

Therefore, in this post, we will discuss the underlying principles of the K-means clustering algorithm including it's mathematical derivation and a step-by-step implementation in Python. We will also discuss the limitations of the algorithm and provide pointer to resources for further reading.

<a href="" target="_blank"></a>
Meauring Distance
======
However, to start, we must first build an inuition of measuring distances between points because this is very important in the K-means clustering and other machine learning algorithms. In this post, we will focus on the <a href="https://en.wikipedia.org/wiki/Euclidean_distance" target="_blank">Euclidean distance</a>, which is the most common distance metric used in machine learning. The Euclidean distance between two points, $x$ and $y$, in an $n$-dimensional real <a href="https://en.wikipedia.org/wiki/Vector_space" target="_blank">vector space</a> is defined as the square root of the sum of the squared differences between the corresponding elements of the two vectors. The Euclidean distance can be expressed as follows:

$$
\begin{equation}
d_E(x,y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}
\end{equation}
$$
where $x_i$ and $y_i$ are the $i$th elements of the vectors $x$ and $y$, respectively. The Euclidean distance is also known as the $L_2$ norm or $L_2$ distance. 



<figure id="Euclidian_Distance">
    <img src="http://yusufbrima.github.io/images/Euclidian_Distance.png" style="height:430px;width:550px;"
         alt="A simple vibration of a physical system in a natural frequency">
    <figcaption> Figure 1: The Euclidean distance is a commonly used measure of similarity between two points in a Euclidean space. It is defined as the square root of the sum of the squared differences between the corresponding coordinates of the two points as shown in this illustration where $\mathbf{a}$ and  $\mathbf{b}$ are two vectors in in $\mathbb{R}^2$. These can be, for example representations of two MRI scans as depicted in this figure.  However, this notion generalizes to $\mathbb{R}^n$.
    </figcaption>
</figure>

The underlying principles
======
$K$-means clustering is a popular unsupervised machine learning algorithm used in various domains, including medical data analysis. The goal of $K$-means clustering is to group similar data points together based on their features. In this algorithm, we start with an initial guess of $K$ centroids of the clusters and then iteratively assign each data point to the nearest centroid based on a distance metric such as Euclidean distance and update the centroid based on the mean of the assigned data points.

The $K$-means algorithm is initialized with the number of clusters, $K$, and the initial centroids are chosen randomly or using some other heuristic approach. Once the initial centroids are chosen, the algorithm alternates between two steps until convergence:
1. Assignment step: In this step, each data point is assigned to the nearest centroid. Specifically, each data point is assigned to the cluster with the nearest centroid based on the Euclidean distance or other distance metric. The assignment step can be formulated as follows:
$$
\begin{equation}
c_i = \text{argmin}_j ||\mathbf{x}_i - \boldsymbol{\mu}_j||^2,
\end{equation}
$$
where $\mathbf{x}_i$ is the $i$th data point, $\boldsymbol{\mu}_j$ is the centroid of the $j$th cluster, and $c_i$ is the index of the cluster that the $i$th data point is assigned to.
2. Update step: In this step, the centroids are updated based on the mean of the data points assigned to each cluster. The update step can be formulated as follows:

$$
\begin{equation}
\boldsymbol{\mu}j = \frac{1}{n_j}\sum{i=1}^{n} \mathbf{x}i \cdot \delta{c_i,j},
\end{equation}
$$

where $n_j$ is the number of data points assigned to the $j$th cluster, $\delta_{c_i,j}$ is the Kronecker delta which is 1 if $c_i=j$ and 0 otherwise, and the sum is taken over all $n$ data points.

<figure id="K_Means_Clustering">
    <img src="http://yusufbrima.github.io/images/K_Means_Clustering.png" style="height:430px;width:550px;"
         alt="A simple vibration of a physical system in a natural frequency">
    <figcaption> Figure 2: A visual illustration of $K$-Means clustering on a $1D$ dataset where we have $n=14$ samples and the cluster centriods, $K=3$. In this formulation, the initial centroids are uniformly randomly chosen as indicated by the vector means $\large{\mu} \sim  \mathcal{U}(\mathcal{D}, K)$ as indicated in step 1. The distance $d_E(x_i, \large{\mu}_i)$ is computed and the sample $x_i$ is assigned to the cluster with the minimum distance as shown in the figure where sample $x_1$ is assigned to cluster $1$. The new cluster centroids as computed from the updated cluster assignment such that the mean $\large{\mu}_k$ of cluster $k$ is all samples that belong to that cluster. This step is repeated until convergence. Finally, all samples have been properly clustered as indicated by step n in the figure where the colors indicate the clusters to which each data point belongs.
    </figcaption>
</figure>

The algorithm iterates between these two steps until convergence, which is typically determined by a stopping criterion such as a maximum number of iterations or when the change in the centroids falls below a certain threshold. We have shown an illustration of this approach to clustering of a $2D$ feature space in <a href="#K_Means_Clustering"> Figure 2</a>.

<figure id="KMeans">
    <img src="http://yusufbrima.github.io/images/KMeans_Before_Clustering.png" style="height:430px;width:380px;"
         alt="Before K-Means Clustering">
    <img src="http://yusufbrima.github.io/images/KMeans_After_Clustering.png" style="height:430px;width:380px;"
         alt="After K-Means Clustering">
    <figcaption> 
    </figcaption>
</figure>

K-means clustering is a computationally efficient algorithm and can handle large datasets. However, its performance can be sensitive to the initial choice of centroids and the number of clusters. Additionally, k-means clustering assumes that clusters are spherical and equally sized, which may not always be the case in practice.

References
======
