---
title: 'K-Means Clustering from First Principles'
date: 2023-05-17
permalink: /posts/2023/05/blog-post-2/
author: Yusuf Brima
tags:
  - Clustering
  - Representation Learning
  - Metric Learning
  - Machine Learning
  - Unsepervised Learning
---
<p class="page__date"><strong>
  <i class="fa fa-fw fa-user" aria-hidden="true"></i> Author:</strong>
  Yusuf Brima
</p>

K-Means Clustering from First Principles
======
$K$-means clustering is a popular unsupervised machine learning algorithm used in various domains, including medical data analysis. The goal of $K$-means clustering is to group similar data points together based on their features. In this algorithm, we start with an initial guess of $K$ centroids of the clusters and then iteratively assign each data point to the nearest centroid based on a distance metric such as Euclidean distance and update the centroid based on the mean of the assigned data points.

The $K$-means algorithm is initialized with the number of clusters, $K$, and the initial centroids are chosen randomly or using some other heuristic approach. Once the initial centroids are chosen, the algorithm alternates between two steps until convergence:
1. Assignment step: In this step, each data point is assigned to the nearest centroid. Specifically, each data point is assigned to the cluster with the nearest centroid based on the Euclidean distance or other distance metric. The assignment step can be formulated as follows:
$$
\begin{equation}
c_i = \text{argmin}_j ||\mathbf{x}_i - \boldsymbol{\mu}_j||^2,
\end{equation}
$$
where $\mathbf{x}_i$ is the $i$th data point, $\boldsymbol{\mu}_j$ is the centroid of the $j$th cluster, and $c_i$ is the index of the cluster that the $i$th data point is assigned to.
2. Update step: In this step, the centroids are updated based on the mean of the data points assigned to each cluster. The update step can be formulated as follows:

$$
\begin{equation}
\boldsymbol{\mu}j = \frac{1}{n_j}\sum{i=1}^{n} \mathbf{x}i \cdot \delta{c_i,j},
\end{equation}
$$

where $n_j$ is the number of data points assigned to the $j$th cluster, $\delta_{c_i,j}$ is the Kronecker delta which is 1 if $c_i=j$ and 0 otherwise, and the sum is taken over all $n$ data points.

<figure id="K_Means_Clustering">
    <img src="http://yusufbrima.github.io/images/K_Means_Clustering.png" style="height:430px;width:550px;"
         alt="A simple vibration of a physical system in a natural frequency">
    <figcaption> Figure 1: A visual illustration of $K$-Means clustering on a $1D$ dataset where we have $n=14$ samples and the cluster centriods, $K=3$. In this formulation, the initial centroids are uniformly randomly chosen as indicated by the vector means $\large{\bm{\mu}} \sim  \mathcal{U}(\mathcal{D}, K)$ as indicated in step 1. The distance $d_E(x_i, \large{\bm{\mu}}_i)$ is computed and the sample $x_i$ is assigned to the cluster with the minimum distance as shown in the figure where sample $x_1$ is assigned to cluster $1$. The new cluster centroids as computed from the updated cluster assignment such that the mean $\large{\bm{\mu}}_k$ of cluster $k$ is all samples that belong to that cluster. This step is repeated until convergence. Finally, all samples have been properly clustered as indicated by step n in the figure where the colors indicate the clusters to which each data point belongs.
    </figcaption>
</figure>

The algorithm iterates between these two steps until convergence, which is typically determined by a stopping criterion such as a maximum number of iterations or when the change in the centroids falls below a certain threshold. We have shown an illustration of this approach to clustering of a $2D$ feature space in <a href="#K_Means_Clustering"> Figure 1</a>.

<figure id="KMeans">
    <img src="http://yusufbrima.github.io/images/KMeans_Before_Clustering.png" style="height:430px;width:550px;"
         alt="Before K-Means Clustering">
    <img src="http://yusufbrima.github.io/images/KMeans_After_Clustering.png" style="height:430px;width:550px;"
         alt="After K-Means Clustering">
    <figcaption> 
    </figcaption>
</figure>

K-means clustering is a computationally efficient algorithm and can handle large datasets. However, its performance can be sensitive to the initial choice of centroids and the number of clusters. Additionally, k-means clustering assumes that clusters are spherical and equally sized, which may not always be the case in practice.

References
======
